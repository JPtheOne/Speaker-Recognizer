{"cells":[{"cell_type":"markdown","source":["# Project 2: Who is talking?\n","## Model 1: Taken from Tensorflow\n","### Obtained 86% Accuracy\n","\n","1.  Ximena Vazquez-Mellado Flores  171319\n","2.  Alejandro Sánchez Gónzalez 167299\n","3.  Ricardo Díaz Mendez 166435\n","4.  Juan Pablo Morales Durante 171657\n"],"metadata":{"id":"7yLe-eleVz1R"}},{"cell_type":"markdown","metadata":{"id":"W7JJ8NAjG0zp"},"source":["# Introduction\n","\n","This documentation offers an in-depth look at our machine learning project 'Who is talking?' This project is dedicated to the development of a robust model aimed at speaker identification. Building upon our prior project, where we recorded diverse audio samples from multiple speakers, the core objective of this work is to develop a model capable of distinguishing individual speakers.\n","\n","This particular program is the second effort where two pre-trained models were used. This program utilizes the pre-trained YAMNet model, obtained from TensorFlow Hub, that is capable of recognizing 521 audio events. We utilize YAMNet to obtain high-quality embeddings from audio sources, extracting vital features from audio data, which we then use to train a custom classifier. To obtain better results, we fine-tune YAMNet for our specific audio classification task, saving the final resulting model into google drive. This program has been designed specifically for voice recognition tasks and can be adapted for various audio classification tasks.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kltPZsM3sYh5"},"source":["# Libraries and dependencies\n","### Dependencies to override the version of Tensorflow and Tensorflow IO\n","\n","We begin the program by installing and importing the necessary extensions that will be used in the code.\n","\n","TensorFlow is an open-source library for numerical computation and machine learning. TensorFlow 2.11 is installed, which is a specific version known for its stability and feature set for deep learning models.\n","TensorFlow I/O is an extension library for TensorFlow that provides support for various file formats and file systems.\n","These two dependencies are installed to have a compatible environment with the YAMNet model.\n","\n","PyDub is a simple and easy-to-use Python library for audio processing. It allows for manipulation of audio with a simple and Pythonic interface.\n","\n","###Imported Libraries:\n","- **os:** Provides a portable way of using operating system-dependent functionality to interact with the file system.\n","- **random:** Implements pseudo-random number generators for various distributions.\n","- **numpy (as np):** Adds support for large, multi-dimensional arrays and matrices, along with a collection of high-level mathematical functions to operate on these arrays.\n","- **pandas (as pd):** Offers data structures and operations for manipulating numerical tables and time series.\n","- **matplotlib.pyplot (as plt):** Provides a MATLAB-like plotting framework for creating static, interactive, and animated visualizations in Python.\n","- **pydub.AudioSegment:** Used for manipulating audio with an easy-to-use interface.\n","- **tensorflow_io (as tfio):** Extends TensorFlow functionality to handle audio processing and I/O operations.\n","- **tensorflow (as tf):** Serves as the backbone of machine learning and neural network operations.\n","- **tensorflow_hub (as hub):** Facilitates the transfer learning by allowing the use of reusable machine learning modules.\n","- **keras.models.load_model:** Used to load a saved Keras model from disk.\n","Data Preprocessing\n","- **sklearn.model_selection.train_test_split:** Utility function to split data arrays into two subsets: for training data and for testing data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HyTxHcOZsoyG","outputId":"5a74a4d8-28ac-4747-8ef4-e1e793111d8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Installing collected packages: pydub\n","Successfully installed pydub-0.25.1\n"]}],"source":["!pip install -q \"tensorflow==2.11.*\"\n","!pip install -q \"tensorflow_io==0.28.*\"\n","!pip install pydub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3OmYHhjsfXQ"},"outputs":[],"source":["import os\n","from IPython import display\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_io as tfio\n","import random\n","from pydub import AudioSegment\n","from sklearn.model_selection import train_test_split\n","from keras.models import load_model"]},{"cell_type":"markdown","metadata":{"id":"yR46Uvm_tGXV"},"source":["# Classes & data\n","\n","In this step, we prepare the audio data for processing and feeding into a machine learning model. This involves defining the classes, loading audio files, segmenting them, and creating a structured dataset.\n","\n","The classes represent different categories or labels in the dataset, each corresponding to a different speaker.\n","\n","A TensorFlow decorator @tf.function is used to define the function \"load_wav_16k_mono\" that efficiently loads audio files as 16kHz mono signals. This function is vital for our model to work correctly since the YAMNet model requires the audio input to be mono and have a sample rate of 16KHz.\n","\n","The \"segment_audios\" function is defined to segment the audio files into one-second clips, save them into a specified folder, and compile metadata about the segments.\n","\n","\"from google.colab import drive\": this line mounts the Google Drive to access the dataset stored in a specified path, which is saved in the variable \"dataset_paths\"\n","segment_audios(\"divided_audios\", dataset_paths)\": The path to the dataset is specified, and the audio segmenting function is called to preprocess the audio files and save them on a local file under the name \"divided_audios\".\n","\n","\"for f in os.listdir(path):\": The audio files are sorted into lists based on the initial characters of the filenames, corresponding to the predefined classes.\n","\"dataframe = pd.DataFrame(data)\": A dictionary and then a dataframe is created to map each audio file to its corresponding class label and a randomly generated key for splitting the dataset later.\n","\"ds = tf.data.Dataset.from_tensor_slices((filenames, labels, keys))\": The lists of filenames and labels are transformed into TensorFlow datasets for efficient input to a machine learning model.\n","\n","The dataset is then mapped using the load_wav_for_map function, which applies the load_wav_16k_mono function to each element of the dataset.\n","\"ds = ds.map(load_wav_for_map)\": With this preparation, the audio dataset is now in a suitable format to be used to train the model, with audio files segmented, categorized, and encoded into tensors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvc8Oha6tA-e"},"outputs":[],"source":["classes = [\"jp\", \"xime\", \"jano\", \"rich\", \"hele\", \"gaby\", \"fede\", \"faro\"]\n","mapped_classes = {\n","    \"jp\": 0,\n","    \"xime\": 1,\n","    \"jano\": 2,\n","    \"rich\": 3,\n","    \"hele\": 4,\n","    \"gaby\": 5,\n","    \"fede\": 6,\n","    \"faro\": 7\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0P4MPGv7Ej_"},"outputs":[],"source":["# -- Utility Function to load file as 16KHz mono --\n","@tf.function\n","def load_wav_16k_mono(filename):\n","    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n","    file_contents = tf.io.read_file(filename)\n","    wav, sample_rate = tf.audio.decode_wav(\n","          file_contents,\n","          desired_channels=1)\n","    wav = tf.squeeze(wav, axis=-1)\n","    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n","    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n","    return wav"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"reL1TMR6xlfb"},"outputs":[],"source":["def segment_audios(output_folder, root_folder):\n","    # Definir la duración de los segmentos en milisegundos (10 segundos = 10000 ms)\n","    duracion_segmento = 1000\n","\n","    # Crear una lista para almacenar la información de los segmentos\n","    segment_info = []\n","\n","    # Crear un directorio para guardar los segmentos si no existe\n","    if not os.path.exists(output_folder):\n","        os.mkdir(output_folder)\n","\n","    i = 0\n","\n","    for folder, _, files in os.walk(root_folder):\n","        for file in files:\n","            if file.endswith(\".wav\"):  # Filtrar archivos de audio, puedes ajustar la extensión según sea necesario\n","                file_path = os.path.join(folder, file)\n","                i = 0\n","                # Obtener el nombre base del archivo para usar como clase\n","                nombre_base = os.path.splitext(os.path.basename(file_path))[0]\n","\n","                # Cargar el archivo de audio original\n","                audio = AudioSegment.from_file(file_path)\n","\n","                # Obtener la duración total del audio en milisegundos\n","                duracion_total = len(audio)\n","\n","                # Dividir el audio en segmentos de 10 segundos y guardar la información\n","                for inicio_ms in range(0, duracion_total, duracion_segmento):\n","                    fin_ms = inicio_ms + duracion_segmento\n","\n","                    # Generar el nombre del archivo para el segmento\n","                    nombre_archivo = f\"{nombre_base}_{i}.wav\"  # {nombre_base}_{i}.wav\n","                    i += 1\n","                    # Guardar el segmento en el directorio !! en un folder por user TODO\n","                    segmento = audio[inicio_ms:fin_ms]\n","                    segmento.export(os.path.join(output_folder, nombre_archivo), format=\"wav\")\n","\n","    print(f\"Segmentos de audio guardados con éxito en '{output_folder}'.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"8ato1mZXycLL","outputId":"8efc25ec-214b-4521-84e4-929d7743258a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Ruta de los archivos de audio\n","dataset_paths = '/content/drive/MyDrive/SelectedTopics/project2'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"MfJyvLYCyCpm","outputId":"5d447fd9-51ff-4adf-989b-ceb3a212c4cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Segmentos de audio guardados con éxito en 'divided_audios'.\n"]}],"source":["!rm -rf /content/divided_audios\n","segment_audios(\"divided_audios\", dataset_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QrR1UCzJ1WH7"},"outputs":[],"source":["# get all audios starting by name\n","path = \"/content/divided_audios\"\n","jp_audios = []\n","xime_audios = []\n","jano_audios = []\n","rich_audios = []\n","hele_audios = []\n","gaby_audios = []\n","fede_audios = []\n","faro_audios = []\n","\n","for f in os.listdir(path):\n","  if f[0] == \"j\" and f[1] == \"p\":\n","    jp_audios.append(f)\n","  elif f[0] == \"x\":\n","    xime_audios.append(f)\n","  elif f[0] == \"r\":\n","    rich_audios.append(f)\n","  elif f[0] == \"h\":\n","    hele_audios.append(f)\n","  elif f[0] == \"g\":\n","    gaby_audios.append(f)\n","  elif f[0] == \"f\" and f[1] == \"e\":\n","    fede_audios.append(f)\n","  elif f[0] == \"f\" and f[1] == \"a\":\n","    faro_audios.append(f)\n","  else:\n","    jano_audios.append(f)\n","\n","# create dataframe with filename, target\n","df = {}\n","for f in os.listdir(path):\n","  if f in jp_audios:\n","    df[f\"/content/divided_audios/{f}\"] = 0\n","  elif f in xime_audios:\n","    df[f\"/content/divided_audios/{f}\"] = 1\n","  elif f in jano_audios:\n","    df[f\"/content/divided_audios/{f}\"] = 2\n","  elif f in rich_audios:\n","    df[f\"/content/divided_audios/{f}\"] = 3\n","  elif f in hele_audios:\n","    df[f\"/content/divided_audios/{f}\"] = 4\n","  elif f in gaby_audios:\n","    df[f\"/content/divided_audios/{f}\"] = 5\n","  elif f in fede_audios:\n","    df[f\"/content/divided_audios/{f}\"] = 6\n","  else:\n","    df[f\"/content/divided_audios/{f}\"] = 7\n","\n","# make key list\n","keys = []\n","for i in range(len(os.listdir(\"/content/divided_audios\"))):\n","  keys.append(random.randint(0,4))\n","\n","data = {\n","    'File': list(df.keys()),\n","    'Label': list(df.values()),\n","    'Key': keys\n","}\n","\n","dataframe = pd.DataFrame(data)\n","\n","# transform into tensors\n","filenames = dataframe[\"File\"]\n","labels = dataframe[\"Label\"]\n","keys = dataframe[\"Key\"]\n","ds = tf.data.Dataset.from_tensor_slices((filenames, labels, keys))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"-Hm71CQl8Wmh","outputId":"2b120a1d-6f12-4d27-84a8-1fd42d97bf1b"},"outputs":[{"data":{"text/plain":["(TensorSpec(shape=(), dtype=tf.string, name=None),\n"," TensorSpec(shape=(), dtype=tf.int64, name=None),\n"," TensorSpec(shape=(), dtype=tf.int64, name=None))"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["ds.element_spec"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"HpSSoKeX_rAd","outputId":"e5f8ed15-a646-4f33-e827-c24139098887"},"outputs":[{"data":{"text/plain":["(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n"," TensorSpec(shape=(), dtype=tf.int64, name=None),\n"," TensorSpec(shape=(), dtype=tf.int64, name=None))"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["def load_wav_for_map(filename, label, key):\n","  return load_wav_16k_mono(filename), label, key\n","\n","ds = ds.map(load_wav_for_map)\n","ds.element_spec"]},{"cell_type":"markdown","metadata":{"id":"0_vKl_i_svzh"},"source":["# Downloading the YAMNet model\n","\n","After that, the code is utilizing the YAMNet model, a deep learning model trained on a large dataset of environmental sounds, to extract features from audio data, followed by training a custom model and performing inference.\n","\n","We begin by setting the URL where the YAMNet model is hosted on TensorFlow Hub to a variable called yamnet_model_handle.\n","\n","The \"extract_embedding\" function applies YAMNet to the audio data to extract embeddings, which are rich representations of the audio features.\n","The dataset is processed to extract features using the map function, which applies the \"extract_embedding\" function to each element."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SoJegTeksuT0"},"outputs":[],"source":["yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n","yamnet_model = hub.load(yamnet_model_handle)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"JO-WNY3YDszd","outputId":"b0b9b488-4a97-471c-d493-197f4991db16"},"outputs":[{"data":{"text/plain":["(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n"," TensorSpec(shape=(), dtype=tf.int64, name=None),\n"," TensorSpec(shape=(), dtype=tf.int64, name=None))"]},"execution_count":102,"metadata":{},"output_type":"execute_result"}],"source":["# applies the embedding extraction model to a wav data\n","def extract_embedding(wav_data, label, key):\n","  ''' run YAMNet to extract embedding from the wav data '''\n","  scores, embeddings, spectrogram = yamnet_model(wav_data)\n","  num_embeddings = tf.shape(embeddings)[0]\n","  return (embeddings, tf.repeat(label, num_embeddings), tf.repeat(key, num_embeddings))\n","\n","# extract embedding\n","ds = ds.map(extract_embedding).unbatch()\n","ds.element_spec"]},{"cell_type":"markdown","source":["# Split data and remove \"keys\" column\n","\n","The dataset, now stored in the variable \"cached_ds\" is split into training, validation, and test datasets based on the value of the keys which act as an identifier for which subset each data point should belong to.\n","With the line \"**train_ds = train_ds.map(remove_fold_column).cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\"** The datasets are further prepared:\n","- train_ds.map(remove_fold_column): This applies the remove_fold_column function to each element in the train_ds dataset. The function removes the keys field from the dataset, which is no longer needed after splitting the data.\n","- .cache(): This method caches the dataset in memory. This is done after mapping and before shuffling, batching, and prefetching to ensure that these operations do not need to be re-executed when the dataset is iterated over multiple epochs. Caching can speed up training by reducing the time spent on data loading and preprocessing during each epoch.\n","- .shuffle(1000): This method randomly shuffles the elements of the dataset. 1000 is the size of the buffer that shuffle will use to sample elements. A large buffer size ensures better randomness but uses more memory, while a smaller buffer size uses less memory but may reduce randomness. The chosen buffer size here is 1000, which means that the shuffling will happen within a window of 1000 elements.\n","- .batch(32): This method combines consecutive elements of the dataset into batches. The argument \"32\" specifies the batch size, meaning each batch will contain 32 elements (i.e., training examples). Batching is required for training neural networks, as it allows for more efficient gradient calculations by processing multiple data points in parallel.\n","- .prefetch(tf.data.AUTOTUNE): This method allows the dataset to prepare subsequent batches while the current batch is being processed. This can improve latency and throughput at the cost of using additional memory to store the prefetched batches. \"tf.data.AUTOTUNE\" is an argument that allows TensorFlow to automatically adjust the number of batches to prefetch dynamically, based on available resources and runtime conditions. This helps to optimize the prefetching process without manual tuning."],"metadata":{"id":"RHlBswfob1aJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfmNAxGoGPJN"},"outputs":[],"source":["### split data\n","cached_ds = ds.cache()\n","train_ds = cached_ds.filter(lambda embedding, label, keys: keys == 0 or keys == 1)\n","val_ds = cached_ds.filter(lambda embedding, label, keys: keys == 2)\n","test_ds = cached_ds.filter(lambda embedding, label, keys: keys == 3)\n","\n","# remove the folds column now that it's not needed anymore\n","remove_fold_column = lambda embedding, label, keys: (embedding, label)\n","\n","train_ds = train_ds.map(remove_fold_column)\n","val_ds = val_ds.map(remove_fold_column)\n","test_ds = test_ds.map(remove_fold_column)\n","\n","train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n","val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n","test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"markdown","source":["# Create, compile and train the new model\n","\n","Now that the dataset has been prepared, the model is defined, compiled and trained using the training and validation datasets.\n","\n","### Creation of the new model\n","Thie first block of code is defining a simple neural network model using TensorFlow's Keras API:\n","- tf.keras.Sequential: This is used to sequentially group a linear stack of layers into a TensorFlow Keras model.\n","- tf.keras.layers.Input: This specifies the input shape that the model will expect. Each input will be a 1D tensor (vector) with 1024 elements, which represents the embedding extracted from audio data by the YAMNet model. The \"dtype\" argument specifies that these elements are floating-point numbers.\n","- tf.keras.layers.Dense(512, activation='relu'): This is a fully connected layer (also known as a dense layer) with 512 neurons. The \"activation='relu'\" argument specifies that the Rectified Linear Unit (ReLU) function should be used as the activation function for each neuron in this layer.\n","- tf.keras.layers.Dense(8): This is another dense layer with 8 neurons, corresponding to the number of classes in the dataset (our classes 'xime', 'jp', 'jano', 'rich', and four others). Since this is the output layer and no activation function is specified, it implies that this model will output the raw scores (logits) for each class, which are typically passed through a softmax function during inference to obtain probabilities.\n","- name='my_model': This names the model \"my_model\", which can be useful for referencing the model later on.\n","\n","### Compiling and training\n","The model is then compiled and trained using the training and validation datasets.\n","- my_model.compile(): This method configures the model for training.\n","- loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True): The loss function is set to Sparse Categorical Crossentropy, which is a standard loss function for multi-class classification problems.\n","The \"from_logits=True\" argument indicates that the outputs of the model are raw logits (not normalized to probabilities by a softmax function).\n","- optimizer=\"adam\": The optimizer is set to 'adam', this option adjusts the weights during training to minimize the loss.\n","- metrics=['accuracy']: The metric for evaluation is set to accuracy, which is the fraction of correctly classified instances among the total number of instances.\n","\n","\n","After training, the model's performance is evaluated on the test dataset.\n","- loss, accuracy = my_model.evaluate(test_ds): The evaluate method computes the loss and accuracy metrics for the dataset provided."],"metadata":{"id":"CEtGOcbkdabn"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"WZ0EpfK1wdLz","outputId":"863d0b2a-ea58-496c-f244-d69d32299b24"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"my_model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_14 (Dense)            (None, 512)               524800    \n","                                                                 \n"," dense_15 (Dense)            (None, 8)                 4104      \n","                                                                 \n","=================================================================\n","Total params: 528,904\n","Trainable params: 528,904\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# create new model\n","my_model = tf.keras.Sequential([\n","    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n","                          name='input_embedding'),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dense(8) # xime, jp, jano, rich, y youtubers\n","], name='my_model')\n","\n","my_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YsFjzLhmxAvj"},"outputs":[],"source":["my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                 optimizer=\"adam\",\n","                 metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"zScahXP4wr55","outputId":"950d25eb-99c9-465f-cffb-936f5a563921"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","193/193 [==============================] - 1s 7ms/step - loss: 0.0107 - accuracy: 0.9994 - val_loss: 1.0335 - val_accuracy: 0.8530\n","Epoch 2/30\n","193/193 [==============================] - 2s 11ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 1.1084 - val_accuracy: 0.8527\n","Epoch 3/30\n","193/193 [==============================] - 2s 13ms/step - loss: 0.0128 - accuracy: 0.9994 - val_loss: 1.0893 - val_accuracy: 0.8499\n","Epoch 4/30\n","193/193 [==============================] - 2s 12ms/step - loss: 0.0296 - accuracy: 0.9981 - val_loss: 1.2219 - val_accuracy: 0.8520\n","Epoch 5/30\n","193/193 [==============================] - 1s 7ms/step - loss: 0.0217 - accuracy: 0.9956 - val_loss: 1.2449 - val_accuracy: 0.8327\n","Epoch 6/30\n","193/193 [==============================] - 1s 8ms/step - loss: 0.0436 - accuracy: 0.9885 - val_loss: 1.1767 - val_accuracy: 0.8330\n","Epoch 7/30\n","193/193 [==============================] - 1s 7ms/step - loss: 0.0293 - accuracy: 0.9932 - val_loss: 1.0790 - val_accuracy: 0.8487\n","Epoch 8/30\n","193/193 [==============================] - 1s 7ms/step - loss: 0.0147 - accuracy: 0.9981 - val_loss: 1.0147 - val_accuracy: 0.8530\n","Epoch 9/30\n","193/193 [==============================] - 1s 7ms/step - loss: 0.0079 - accuracy: 0.9992 - val_loss: 1.0273 - val_accuracy: 0.8493\n","Epoch 10/30\n","193/193 [==============================] - 2s 12ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 1.0194 - val_accuracy: 0.8511\n","Epoch 11/30\n","193/193 [==============================] - 2s 13ms/step - loss: 0.0305 - accuracy: 0.9990 - val_loss: 1.0547 - val_accuracy: 0.8517\n","Epoch 12/30\n","193/193 [==============================] - 2s 13ms/step - loss: 7.6518e-04 - accuracy: 0.9997 - val_loss: 1.0379 - val_accuracy: 0.8517\n","Epoch 13/30\n","193/193 [==============================] - 1s 8ms/step - loss: 0.0212 - accuracy: 0.9994 - val_loss: 1.0436 - val_accuracy: 0.8484\n","Epoch 14/30\n","193/193 [==============================] - 1s 7ms/step - loss: 0.0310 - accuracy: 0.9984 - val_loss: 1.0995 - val_accuracy: 0.8450\n","Epoch 15/30\n","193/193 [==============================] - 1s 7ms/step - loss: 0.0076 - accuracy: 0.9990 - val_loss: 1.1603 - val_accuracy: 0.8480\n","Epoch 16/30\n","193/193 [==============================] - 1s 8ms/step - loss: 0.0350 - accuracy: 0.9934 - val_loss: 1.1477 - val_accuracy: 0.8361\n","Epoch 17/30\n","193/193 [==============================] - 1s 8ms/step - loss: 0.0237 - accuracy: 0.9955 - val_loss: 1.3328 - val_accuracy: 0.8317\n","Epoch 18/30\n","193/193 [==============================] - 2s 12ms/step - loss: 0.0393 - accuracy: 0.9945 - val_loss: 1.1347 - val_accuracy: 0.8468\n","Epoch 19/30\n","193/193 [==============================] - 2s 13ms/step - loss: 0.0064 - accuracy: 0.9987 - val_loss: 1.0800 - val_accuracy: 0.8539\n","Epoch 20/30\n","193/193 [==============================] - 2s 9ms/step - loss: 0.0307 - accuracy: 0.9958 - val_loss: 1.0940 - val_accuracy: 0.8490\n","Epoch 21/30\n","193/193 [==============================] - 1s 7ms/step - loss: 0.0414 - accuracy: 0.9974 - val_loss: 1.1419 - val_accuracy: 0.8508\n","Epoch 22/30\n","193/193 [==============================] - 1s 7ms/step - loss: 0.0161 - accuracy: 0.9985 - val_loss: 1.1942 - val_accuracy: 0.8444\n","Epoch 23/30\n","193/193 [==============================] - 2s 8ms/step - loss: 0.0492 - accuracy: 0.9938 - val_loss: 1.1726 - val_accuracy: 0.8456\n","Epoch 24/30\n","193/193 [==============================] - 1s 7ms/step - loss: 0.0238 - accuracy: 0.9974 - val_loss: 1.1646 - val_accuracy: 0.8468\n","Epoch 25/30\n","193/193 [==============================] - 2s 12ms/step - loss: 0.0210 - accuracy: 0.9969 - val_loss: 1.1720 - val_accuracy: 0.8480\n","Epoch 26/30\n","193/193 [==============================] - 2s 11ms/step - loss: 0.0376 - accuracy: 0.9969 - val_loss: 1.1912 - val_accuracy: 0.8505\n","Epoch 27/30\n","193/193 [==============================] - 2s 13ms/step - loss: 0.0222 - accuracy: 0.9976 - val_loss: 1.1914 - val_accuracy: 0.8465\n","Epoch 28/30\n","193/193 [==============================] - 1s 7ms/step - loss: 0.0133 - accuracy: 0.9989 - val_loss: 1.2169 - val_accuracy: 0.8505\n","Epoch 29/30\n","193/193 [==============================] - 1s 7ms/step - loss: 0.0232 - accuracy: 0.9989 - val_loss: 1.1379 - val_accuracy: 0.8527\n","Epoch 30/30\n","193/193 [==============================] - 1s 7ms/step - loss: 0.0343 - accuracy: 0.9985 - val_loss: 1.2125 - val_accuracy: 0.8517\n"]}],"source":["history = my_model.fit(train_ds,\n","                       epochs=30,\n","                       validation_data=val_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"_nRzqtE4wEXf","outputId":"d792e130-bd00-4e66-b9ca-21ef4ee84b6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["98/98 [==============================] - 0s 3ms/step - loss: 1.0291 - accuracy: 0.8669\n","Loss: 1.0290833711624146\n","Accuracy: 86.69%\n"]}],"source":["loss, accuracy = my_model.evaluate(test_ds)\n","\n","print(f\"Loss: {loss}\")\n","print(f\"Accuracy: {accuracy*100:.2f}%\")"]},{"cell_type":"markdown","source":["# Final test of our new model\n","\n","The function predict_audio_class is designed to take an audio file as input and predict which class the audio belongs to:\n","- \"testing_wav_data = load_wav_16k_mono(audio_file)\": This line calls the load_wav_16k_mono function to load the WAV file specified by audio_file, converting it to a mono-channel (single channel) audio with a sample rate of 16 kHz. The audio data is returned as a tensor.\n","- \"scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\": Here, the yamnet_model is used to process the audio data and it returns three values:\n","  - scores: This is the output of the softmax layer of YAMNet, representing the probability of each AudioSet class.\n","  - embeddings: These are the 1024-dimensional embeddings extracted from the audio. They serve as a compact representation of the audio's features.\n","  - spectrogram: This is the mel-spectrogram used by YAMNet as part of its feature extraction process.\n","- \"result = my_model(embeddings).numpy()\": The extracted embeddings are then passed to my_model to get the predictions. The output result is converted to a NumPy array for easier manipulation.\n","- \"inferred_class = classes[result.mean(axis=0).argmax()]\": This line calculates the mean of the results across the time dimension (axis=0), which might be necessary if the model output includes time-distributed predictions, and finds the index of the highest score which corresponds to the most likely class prediction. This index is then used to find the actual class label from the classes list.\n","- \"return f\"{audio_file} audiofile is most similar to {inferred_class}.\"\": When finished, the function returns a formatted string stating which class the audio file is most similar to, based on the model's inference.\n","\n","The prediction function is applied to the dataset of test files to infer the class.\n","The test_files list contains the file paths to the audio files that we want to classify. The for loop iterates through this list, and for each path in test_files, it calls the predict_audio_class function. This function processes the audio file and predicts which class it belongs to. The prediction result is then printed to the console.\n","\n","Finally, the trained model is saved to Google Drive for later use."],"metadata":{"id":"qJSF3OOjfGfz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aQ20JiSxiMj"},"outputs":[],"source":["def predict_audio_class(audio_file):\n","    testing_wav_data = load_wav_16k_mono(audio_file)\n","    scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\n","    result = my_model(embeddings).numpy()\n","\n","    inferred_class = classes[result.mean(axis=0).argmax()]\n","    return f\"{audio_file} audiofile is most similar to {inferred_class}.\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"OV6kR7y9xPa2","outputId":"fdf10d10-0db8-4378-996b-74511703cda6"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/SelectedTopics/project2/test/janoT.wav audiofile is most similar to jano.\n","/content/drive/MyDrive/SelectedTopics/project2/test/ximeT.wav audiofile is most similar to xime.\n","/content/drive/MyDrive/SelectedTopics/project2/test/jpT.wav audiofile is most similar to jp.\n","/content/drive/MyDrive/SelectedTopics/project2/test/richT.wav audiofile is most similar to rich.\n"]}],"source":["test_files = [\n","    '/content/drive/MyDrive/SelectedTopics/project2/test/janoT.wav',\n","    '/content/drive/MyDrive/SelectedTopics/project2/test/ximeT.wav',\n","    '/content/drive/MyDrive/SelectedTopics/project2/test/jpT.wav',\n","    '/content/drive/MyDrive/SelectedTopics/project2/test/richT.wav'\n","]\n","\n","for test in test_files:\n","  print(predict_audio_class(test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BO4bnO08FRXX"},"outputs":[],"source":["# Define the path where you want to save the model in your Google Drive\n","model_save_path = '/content/drive/MyDrive/SelectedTopics/project2/yamnet-final.h5'\n","\n","# Save the Keras model\n","my_model.save(model_save_path)"]},{"cell_type":"markdown","metadata":{"id":"D1Zt89qHG0z5"},"source":["## Conclusion\n","In this project, we developed an audio classification model capable of identifying individual speakers from a dataset of voice recordings. Utilizing TensorFlow and the YAMNet model from TensorFlow Hub, we processed audio files to extract meaningful embeddings, which served as feature representations for our classification task.\n","\n","The dataset was carefully segmented into 1-second audio clips to standardize the input and focus on short-term audio features. These segments were then used to train a neural network model, which included dense layers following the YAMNet embeddings extraction. The model's performance was evaluated based on its accuracy in classifying unseen data, and the results were promising."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}